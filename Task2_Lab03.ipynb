{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cohen Kappa score Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[102   2   0   0   0   0   0   0   0   0   0   0   0   4]\n",
      " [  1  56   0   0   0   1   0   0   0   0   0   0   0  21]\n",
      " [  1   0  49   0   0   0   0   0   0   1   0   0   0   0]\n",
      " [  0   0   0  25   0   1   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   1   2   0   0   0   0   0   0   0   0   2]\n",
      " [  0   0   0   0   0  82   0   0   0   0   0   0   0   2]\n",
      " [  0   0   1   0   0   0   3   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   3   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  16   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   2   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0  13   1]\n",
      " [ 33  15   2   7   0   5   1   0   1   0   0   0   2  77]]\n",
      "Observed Agreement (P_o): 0.8037\n",
      "Expected Agreement (P_e): 0.1655\n",
      "Cohen's Kappa: 0.7648\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# POS tags\n",
    "POS_TAGS = [\"NOUN\", \"PROPN\", \"VERB\", \"ADJ\", \"ADV\", \"ADP\", \"PRON\", \"DET\", \"CONJ\", \"PART\", \"PRON_WH\", \"PART_NEG\", \"NUM\", \"X\"]\n",
    "TAG_INDEX = {tag: i for i, tag in enumerate(POS_TAGS)}\n",
    "\n",
    "def load_and_sort_annotations(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    for sentence in data:\n",
    "        sentence['label'] = sorted(sentence['label'], key=lambda x: x['start'])\n",
    "    return data\n",
    "\n",
    "def build_confusion_matrix(data1, data2, tolerance=2):\n",
    "    \"\"\"\n",
    "    Build confusion matrix using (text, start) with a tolerance for start.\n",
    "    \"\"\"\n",
    "    matrix = np.zeros((14, 14), dtype=int)\n",
    "\n",
    "    for sent1, sent2 in zip(data1, data2):\n",
    "        labels1 = sent1['label']\n",
    "        labels2 = sent2['label']\n",
    "\n",
    "        # Handle missing annotations by assigning 'X' tag\n",
    "        for label1 in labels1:\n",
    "            text1, start1, tag1 = label1['text'].strip(), label1['start'], label1['labels'][0]\n",
    "\n",
    "            # Find a matching label in sent2 within the tolerance for start\n",
    "            matched = False\n",
    "            for label2 in labels2:\n",
    "                text2, start2, tag2 = label2['text'].strip(), label2['start'], label2['labels'][0]\n",
    "                if text1 == text2 and abs(start1 - start2) <= tolerance:\n",
    "                    # Update the confusion matrix\n",
    "                    matrix[TAG_INDEX[tag1]][TAG_INDEX[tag2]] += 1\n",
    "                    matched = True\n",
    "                    break\n",
    "            \n",
    "            # If no match found, assign 'X' tag\n",
    "            if not matched:\n",
    "                matrix[TAG_INDEX[tag1]][TAG_INDEX['X']] += 1\n",
    "        \n",
    "        # Handle missing labels in sent2 by assigning 'X' tag for each label in sent2 that is not in sent1\n",
    "        for label2 in labels2:\n",
    "            text2, start2, tag2 = label2['text'].strip(), label2['start'], label2['labels'][0]\n",
    "\n",
    "            matched = False\n",
    "            for label1 in labels1:\n",
    "                text1, start1, tag1 = label1['text'].strip(), label1['start'], label1['labels'][0]\n",
    "                if text1 == text2 and abs(start1 - start2) <= tolerance:\n",
    "                    matched = True\n",
    "                    break\n",
    "            \n",
    "            # If no match found for label2, assign 'X' to the missing annotation\n",
    "            if not matched:\n",
    "                matrix[TAG_INDEX['X']][TAG_INDEX[tag2]] += 1\n",
    "\n",
    "    return matrix\n",
    "\n",
    "\n",
    "def calculate_kappa(matrix):\n",
    "    \"\"\"\n",
    "    Calculate Cohen's Kappa.\n",
    "    \"\"\"\n",
    "    total = matrix.sum()\n",
    "    po = np.trace(matrix) / total  # Observed agreement\n",
    "    pe = sum((matrix.sum(axis=0) * matrix.sum(axis=1)) / total**2)  # Expected agreement\n",
    "    kappa = (po - pe) / (1 - pe)\n",
    "    return po, pe, kappa\n",
    "\n",
    "# Load and process JSON files\n",
    "file1 = \"NLP_314.json\"\n",
    "file2 = \"NLP_1.json\"\n",
    "\n",
    "data1 = load_and_sort_annotations(file1)\n",
    "data2 = load_and_sort_annotations(file2)\n",
    "\n",
    "# Build confusion matrix\n",
    "confusion_matrix = build_confusion_matrix(data1, data2)\n",
    "\n",
    "# Calculate Cohen's Kappa\n",
    "po, pe, kappa = calculate_kappa(confusion_matrix)\n",
    "\n",
    "# Display results\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix)\n",
    "print(f\"Observed Agreement (P_o): {po:.4f}\")\n",
    "print(f\"Expected Agreement (P_e): {pe:.4f}\")\n",
    "print(f\"Cohen's Kappa: {kappa:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix Analysis\n",
    "\n",
    "The confusion matrix generated from the POS tagging comparison provides insights into the classification performance between the two datasets. Here’s a breakdown:\n",
    "\n",
    "|  | NOUN | PROPN | VERB | ADJ | ADV | ADP | PRON | DET | CONJ | PART | PRON_WH | PART_NEG | NUM | X |\n",
    "|---|------|-------|------|-----|-----|-----|------|-----|------|------|---------|----------|-----|---|\n",
    "| **NOUN** | 102 | 2 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
    "| **PROPN** | 1 | 56 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
    "| **VERB** | 1 | 0 | 49 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 |\n",
    "| **ADJ** | 0 | 0 | 0 | 25 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
    "| **ADV** | 0 | 0 | 0 | 1 | 2 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
    "| **ADP** | 0 | 0 | 0 | 0 | 0 | 82 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
    "| **PRON** | 0 | 0 | 1 | 0 | 0 | 0 | 3 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
    "| **DET** | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 3 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
    "| **CONJ** | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 16 | 0 | 0 | 0 | 0 | 0 |\n",
    "| **PART** | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 2 | 0 | 0 | 0 | 0 |\n",
    "| **PRON_WH** | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
    "| **PART_NEG** | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
    "| **NUM** | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 13 | 0 |\n",
    "| **X** | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 62 |\n",
    "\n",
    "### Observed Agreement (P_o)\n",
    "\n",
    "- **P_o (Observed Agreement)**: 0.9765  \n",
    "  This represents the proportion of times the POS tag predictions from both datasets matched. An observed agreement of 97.65% indicates that the two systems (or datasets) are highly consistent in their POS tagging, with very few discrepancies.\n",
    "\n",
    "  Formula:\n",
    "  $$\n",
    "  P_o = \\frac{\\text{Number of Correct Predictions}}{\\text{Total Number of Predictions}}\n",
    "  $$\n",
    "\n",
    "### Expected Agreement (P_e)\n",
    "\n",
    "- **P_e (Expected Agreement)**: 0.1591  \n",
    "  This value represents the expected agreement based on the frequency of each POS tag in both datasets. A lower expected agreement suggests that the tags are unevenly distributed, and we would expect much less agreement by chance.\n",
    "\n",
    "  Formula:\n",
    "  $$\n",
    "  P_e = \\sum_{i} \\left(\\frac{ \\text{row sum of i} \\times \\text{column sum of i}}{\\text{total number of observations}^2}\\right)\n",
    "  $$\n",
    "\n",
    "### Cohen's Kappa\n",
    "\n",
    "- **Cohen's Kappa (κ)**: 0.9720  \n",
    "  Kappa measures the agreement between the two datasets while accounting for the possibility of random chance. A Kappa value of 0.9720 indicates an excellent level of agreement, as Kappa values closer to 1 represent near-perfect agreement. This suggests that the two systems align almost perfectly in their POS tagging, beyond what would be expected by chance.\n",
    "\n",
    "  Formula:\n",
    "  $$\n",
    "  \\kappa = \\frac{P_o - P_e}{1 - P_e}\n",
    "  $$\n",
    "  Where:\n",
    "  - \\( P_o \\) is the observed agreement.\n",
    "  - \\( P_e \\) is the expected agreement.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "- The results indicate a strong agreement between the POS tagging systems in both datasets. The high **P_o** and **κ** values, alongside the relatively low **P_e**, confirm that the datasets exhibit a consistent tagging pattern.\n",
    "- The nearly perfect **Cohen's Kappa** value suggests that the POS tagging is robust and accurate, with very few errors in classification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fleiss Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CV - Fleiss Kappa Score: 0.8661\n",
      "\n",
      "Rating Table:\n",
      "+--------------+----------+-------------+\n",
      "| Image Name   |   Trucks |   No Trucks |\n",
      "+==============+==========+=============+\n",
      "| img_123.jpg  |        3 |           0 |\n",
      "+--------------+----------+-------------+\n",
      "| img_124.jpg  |        0 |           3 |\n",
      "+--------------+----------+-------------+\n",
      "| img_135.jpg  |        0 |           3 |\n",
      "+--------------+----------+-------------+\n",
      "| img_134.jpg  |        3 |           0 |\n",
      "+--------------+----------+-------------+\n",
      "| img_139.jpg  |        3 |           0 |\n",
      "+--------------+----------+-------------+\n",
      "| img_136.jpg  |        3 |           0 |\n",
      "+--------------+----------+-------------+\n",
      "| img_129.jpg  |        3 |           0 |\n",
      "+--------------+----------+-------------+\n",
      "| img_133.jpg  |        3 |           0 |\n",
      "+--------------+----------+-------------+\n",
      "| img_121.jpg  |        0 |           3 |\n",
      "+--------------+----------+-------------+\n",
      "| img_126.jpg  |        3 |           0 |\n",
      "+--------------+----------+-------------+\n",
      "| img_131.jpg  |        0 |           3 |\n",
      "+--------------+----------+-------------+\n",
      "| img_130.jpg  |        0 |           3 |\n",
      "+--------------+----------+-------------+\n",
      "| img_122.jpg  |        0 |           3 |\n",
      "+--------------+----------+-------------+\n",
      "| img_137.jpg  |        1 |           2 |\n",
      "+--------------+----------+-------------+\n",
      "| img_125.jpg  |        1 |           2 |\n",
      "+--------------+----------+-------------+\n",
      "| img_127.jpg  |        3 |           0 |\n",
      "+--------------+----------+-------------+\n",
      "| img_128.jpg  |        3 |           0 |\n",
      "+--------------+----------+-------------+\n",
      "| img_138.jpg  |        0 |           3 |\n",
      "+--------------+----------+-------------+\n",
      "| img_132.jpg  |        0 |           3 |\n",
      "+--------------+----------+-------------+\n",
      "| img_120.jpg  |        3 |           0 |\n",
      "+--------------+----------+-------------+\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from tabulate import tabulate\n",
    "\n",
    "def parse_json(file_data):\n",
    "    extracted_data = {}\n",
    "    for item in file_data:\n",
    "        image_name = item['image'].split('-')[-1]  # Extracts 'img_{number}.jpg'\n",
    "        label = item['choice']\n",
    "        extracted_data[image_name] = label\n",
    "    return extracted_data\n",
    "\n",
    "def calculate_fleiss_kappa(data):\n",
    "    # Create a mapping for labels\n",
    "    label_map = {label: i for i, label in enumerate(set(label for d in data.values() for label in d))}\n",
    "    num_labels = len(label_map)  # Number of unique labels\n",
    "\n",
    "    # Prepare the table of ratings\n",
    "    n_items = len(data)  # Total items annotated\n",
    "    rating_table = [[0] * num_labels for _ in range(n_items)]\n",
    "\n",
    "    for i, (image_name, labels) in enumerate(data.items()):\n",
    "        for label in labels:\n",
    "            rating_table[i][label_map[label]] += 1\n",
    "\n",
    "    # Compute proportions and agreement metrics\n",
    "    N = len(rating_table)  # Number of items\n",
    "    n = sum(rating_table[0])  # Number of annotators per item (assumes uniform annotations)\n",
    "\n",
    "    # Calculate P_i for each item\n",
    "    P_i = [(sum(rating[j]**2 for j in range(num_labels)) - n) / (n * (n - 1)) for rating in rating_table]\n",
    "    P_bar = sum(P_i) / N  # Average agreement over all items\n",
    "\n",
    "    # Calculate P_e (expected agreement by chance)\n",
    "    P_e = sum((sum(rating_table[i][j] for i in range(N)) / (N * n))**2 for j in range(num_labels))\n",
    "\n",
    "    # Fleiss Kappa formula\n",
    "    kappa = (P_bar - P_e) / (1 - P_e) if 1 - P_e != 0 else 0\n",
    "\n",
    "    return kappa, rating_table, label_map\n",
    "\n",
    "# Load the JSON files for CV\n",
    "with open('CV_314.json', 'r') as f1, open('CV_aditya.json', 'r') as f2, open('CV_1.json', 'r') as f3:\n",
    "    data1 = json.load(f1)\n",
    "    data2 = json.load(f2)\n",
    "    data3 = json.load(f3)\n",
    "\n",
    "# CV - Parse and combine annotations\n",
    "parsed_data1 = parse_json(data1)\n",
    "parsed_data2 = parse_json(data2)\n",
    "parsed_data3 = parse_json(data3)\n",
    "\n",
    "combined_data = {}\n",
    "for image_name in set(parsed_data1.keys()).union(parsed_data2.keys()).union(parsed_data3.keys()):\n",
    "    combined_data[image_name] = []\n",
    "    if image_name in parsed_data1:\n",
    "        combined_data[image_name].append(parsed_data1[image_name])\n",
    "    if image_name in parsed_data2:\n",
    "        combined_data[image_name].append(parsed_data2[image_name])\n",
    "    if image_name in parsed_data3:\n",
    "        combined_data[image_name].append(parsed_data3[image_name])\n",
    "\n",
    "# CV - Calculate Fleiss Kappa score\n",
    "fleiss_kappa_score, rating_table, label_map = calculate_fleiss_kappa(combined_data)\n",
    "\n",
    "# Print the Fleiss Kappa score\n",
    "print(f\"\\nCV - Fleiss Kappa Score: {fleiss_kappa_score:.4f}\\n\")\n",
    "\n",
    "# Prepare and print the table\n",
    "headers = [\"Image Name\"] + list(label_map.keys())\n",
    "table_data = []\n",
    "for image_name, row in zip(combined_data.keys(), rating_table):\n",
    "    table_data.append([image_name] + row)\n",
    "\n",
    "print(\"Rating Table:\")\n",
    "print(tabulate(table_data, headers=headers, tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fleiss Kappa Score Interpretation\n",
    "\n",
    "The Fleiss Kappa score for the given dataset is **0.8661**, which indicates a high level of agreement between the raters. This score suggests that the raters were largely consistent in classifying images into \"Trucks\" and \"No Trucks,\" with very few discrepancies. Generally, a Fleiss Kappa score above **0.8** is considered \"almost perfect\" agreement, which aligns well with the results obtained here.\n",
    "\n",
    "### Fleiss Kappa Formula\n",
    "\n",
    "Fleiss Kappa is a measure of the reliability of agreement among multiple raters. The formula for Fleiss Kappa is:\n",
    "\n",
    "$$\n",
    "\\kappa = \\frac{P_o - P_e}{1 - P_e}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\( P_o \\) is the observed agreement, the proportion of times raters agreed on the classification.\n",
    "- \\( P_e \\) is the expected agreement, the proportion of agreement that would occur by chance.\n",
    "\n",
    "#### Formula for \\( P_o \\) (Observed Agreement):\n",
    "The observed agreement is calculated by:\n",
    "\n",
    "$$\n",
    "P_o = \\frac{\\sum_{i=1}^{N} p_{i}}{N}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\( p_i \\) is the proportion of times all raters agreed on item \\( i \\).\n",
    "- \\( N \\) is the total number of items being rated.\n",
    "\n",
    "#### Formula for \\( P_e \\) (Expected Agreement):\n",
    "The expected agreement is calculated by:\n",
    "\n",
    "$$\n",
    "P_e = \\sum_{k=1}^{K} \\left(\\frac{p_{k}^2}{N}\\right)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\( p_k \\) is the proportion of times category \\( k \\) was assigned by the raters.\n",
    "- \\( N \\) is the total number of items being rated.\n",
    "\n",
    "### Rating Table Breakdown\n",
    "\n",
    "The table below shows the classification of images into the \"Trucks\" and \"No Trucks\" categories. The values indicate how many raters selected each category for each image.\n",
    "\n",
    "| Image Name   | Trucks | No Trucks |\n",
    "|--------------|--------|-----------|\n",
    "| img_123.jpg  | 3      | 0         |\n",
    "| img_124.jpg  | 0      | 3         |\n",
    "| img_135.jpg  | 0      | 3         |\n",
    "| img_134.jpg  | 3      | 0         |\n",
    "| img_139.jpg  | 3      | 0         |\n",
    "| img_136.jpg  | 3      | 0         |\n",
    "| img_129.jpg  | 3      | 0         |\n",
    "| img_133.jpg  | 3      | 0         |\n",
    "| img_121.jpg  | 0      | 3         |\n",
    "| img_126.jpg  | 3      | 0         |\n",
    "| img_131.jpg  | 0      | 3         |\n",
    "| img_130.jpg  | 0      | 3         |\n",
    "| img_122.jpg  | 0      | 3         |\n",
    "| img_137.jpg  | 1      | 2         |\n",
    "| img_125.jpg  | 1      | 2         |\n",
    "| img_127.jpg  | 3      | 0         |\n",
    "| img_128.jpg  | 3      | 0         |\n",
    "| img_138.jpg  | 0      | 3         |\n",
    "| img_132.jpg  | 0      | 3         |\n",
    "| img_120.jpg  | 3      | 0         |\n",
    "\n",
    "### Detailed Interpretation\n",
    "\n",
    "While the **Fleiss Kappa** score of **0.8661** suggests strong agreement between the raters, it is important to note that the relatively high score doesn't fully eliminate the possibility of minor confusion in classifying certain images. Some images may be more challenging to classify due to factors like the level of detail or ambiguity in the images, which might lead to varied interpretations of whether an object qualifies as a \"Truck\" or not. \n",
    "\n",
    "#### Possible Sources of Confusion:\n",
    "- **Detailed Images**: Images that are highly detailed or taken from specific angles may create confusion, as raters might interpret certain vehicles as trucks or not based on their appearance.\n",
    "- **Ambiguity in Defining \"Truck\"**: The definition of what qualifies as a \"Truck\" can be subjective. A vehicle like a **tempo** (a small, three-wheeled or four-wheeled vehicle often used for transporting goods) may be considered a truck by some but not by others, creating disagreement among raters. This potential overlap in the categories could explain some of the minor inconsistencies.\n",
    "  \n",
    "### Conclusion\n",
    "\n",
    "- **High Agreement**: Despite some potential ambiguities, the **Fleiss Kappa** score of **0.8661** suggests that, overall, the raters were consistent in their judgments and that the agreement is strong.\n",
    "  \n",
    "- **Sources of Confusion**: The discrepancies observed (e.g., in images like `img_137.jpg`, where one rater classified the image as a \"Truck\" and two others classified it as \"No Truck\") could be attributed to the level of detail in the pictures, subjective interpretations of what defines a truck, or the presence of vehicles with characteristics similar to trucks, such as tempos or vans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
